{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'output/output/dogscats/'\n",
    "train_filenames = os.listdir(path+'train/')\n",
    "train_cat = filter(lambda x:x[:3] == 'cat', train_filenames)\n",
    "train_dog = filter(lambda x:x[:3] == 'dog', train_filenames)\n",
    "x = ['train_cat', 'train_dog', 'test']\n",
    "y = [len(os.listdir(path+'train/cats/')), len(os.listdir(path+'train/dogs/')), len(os.listdir(path+'test_small/cats/'))]\n",
    "ax = sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain = os.listdir(path+'train/cats/')\n",
    "mytrain += os.listdir(path+'train/dogs/')\n",
    "myvalid = os.listdir(path+'valid/cats/')\n",
    "myvalid += os.listdir(path+'valid/dogs/')\n",
    "\n",
    "print len(mytrain), len(myvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain_cat = filter(lambda x:x[:3] == 'cat', mytrain)\n",
    "mytrain_dog = filter(lambda x:x[:3] == 'dog', mytrain)\n",
    "myvalid_cat = filter(lambda x:x[:3] == 'cat', myvalid)\n",
    "myvalid_dog = filter(lambda x:x[:3] == 'dog', myvalid)\n",
    "x = ['mytrain_cat', 'mytrain_dog', 'myvalid_cat', 'myvalid_dog']\n",
    "y = [len(mytrain_cat), len(mytrain_dog), len(myvalid_cat), len(myvalid_dog)]\n",
    "\n",
    "ax = sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def remove_and_create_class(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)\n",
    "    os.mkdir(dirname+'/cats')\n",
    "    os.mkdir(dirname+'/dogs')\n",
    "\n",
    "remove_and_create_class('mytrain')\n",
    "remove_and_create_class('myvalid')\n",
    "\n",
    "for filename in mytrain_cat:\n",
    "    os.symlink('output/output/dogscats/'+'train/cats/'+filename, 'mytrain/cats/'+filename)\n",
    "\n",
    "for filename in mytrain_dog:\n",
    "    os.symlink('output/output/dogscats/'+'train/dogs/'+filename, 'mytrain/dogs/'+filename)\n",
    "\n",
    "for filename in myvalid_cat:\n",
    "    os.symlink('output/output/dogscats/'+'valid/cats/'+filename, 'myvalid/cats/'+filename)\n",
    "\n",
    "for filename in myvalid_dog:\n",
    "    os.symlink('output/output/dogscats/'+'valid/dogs/'+filename, 'myvalid/dogs/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, add, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD,Adam,Adagrad\n",
    "from keras.utils.data_utils import get_file\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    " \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "image_size = (image_width, image_height)\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'mytrain',  # this is the target directory\n",
    "        target_size=image_size,  # all images will be resized to 224x224\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'myvalid',  # this is the target directory\n",
    "        target_size=image_size,  # all images will be resized to 224x224\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(input_tensor=Input((224, 224, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "    \n",
    "print base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_features = np.zeros(shape=(22000, 7, 7, 512))\n",
    "train_labels = np.zeros(shape=(22000,2))\n",
    "batch_size = 16\n",
    "i = 0\n",
    "\n",
    "for inputs_batch, labels_batch in train_generator :\n",
    "    features_batch = base_model.predict(inputs_batch)\n",
    "    train_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "    train_labels[i * batch_size : (i + 1) * batch_size] = to_categorical(labels_batch)\n",
    "    i += 1\n",
    "    if i * batch_size >= 22000:\n",
    "        break\n",
    "         \n",
    "\n",
    "train_features = np.reshape(train_features, (22000, 7 * 7 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = np.zeros(shape=(2000, 7, 7, 512))\n",
    "valid_labels = np.zeros(shape=(2000,2))\n",
    "batch_size = 16\n",
    "i = 0\n",
    "\n",
    "for inputs_batch, labels_batch in validation_generator :\n",
    "    features_batch = base_model.predict(inputs_batch)\n",
    "    valid_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "    valid_labels[i * batch_size : (i + 1) * batch_size] = to_categorical(labels_batch)\n",
    "    i += 1\n",
    "    if i * batch_size >= 2000:\n",
    "        break\n",
    "        \n",
    "         \n",
    "\n",
    "valid_features = np.reshape(valid_features, (2000, 7 * 7 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "image_size = (image_width, image_height)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'mytest-lbl',  # this is the target directory\n",
    "        target_size=image_size,  # all images will be resized to 224x224\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_features = np.zeros(shape=(1000, 7, 7, 512))\n",
    "test_labels = np.zeros(shape=(1000,))\n",
    "batch_size = 20\n",
    "i = 0\n",
    "\n",
    "for inputs_batch,labels_batch in test_generator :\n",
    "    features_batch = base_model.predict(inputs_batch)\n",
    "    test_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "    test_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "    i += 1\n",
    "    if i * batch_size >= 1000:\n",
    "        break\n",
    "   \n",
    "\n",
    "test_features = np.reshape(test_features, (1000, 7 * 7 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_vgg.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"initial_weights_vgg.h5\")\n",
    "\n",
    "json_file = open('model_vgg.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model0 = model_from_json(loaded_model_json)\n",
    "model0.load_weights(\"initial_weights_vgg.h5\")\n",
    "model0.compile(optimizer=SGD(lr=0.002),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h=model0.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model0.save(\"sgd_vgg_1.h5\")\n",
    "with open('file_sgd_vgg_1.json', 'w') as f:\n",
    "    json.dump(h.history, f)\n",
    "\n",
    "model1 = model_from_json(loaded_model_json)\n",
    "model1.load_weights(\"initial_weights_vgg.h5\")\n",
    "model1.compile(optimizer=SGD(lr=0.001),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h1=model1.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model1.save(\"sgd_vgg_2.h5\")\n",
    "with open('file_sgd_vgg_2.json', 'w') as f:\n",
    "    json.dump(h1.history, f)\n",
    "\n",
    "model2 = model_from_json(loaded_model_json)\n",
    "model2.load_weights(\"initial_weights_vgg.h5\")\n",
    "model2.compile(optimizer=SGD(lr=0.0005),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h2=model2.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model2.save(\"sgd_vgg_3.h5\")\n",
    "with open('file_sgd_vgg_3.json', 'w') as f:\n",
    "    json.dump(h2.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model3 = model_from_json(loaded_model_json)\n",
    "model3.load_weights(\"initial_weights_vgg.h5\")\n",
    "model3.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h3=model3.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model3.save(\"adam_vgg_4.h5\")\n",
    "\n",
    "model4 = model_from_json(loaded_model_json)\n",
    "model4.load_weights(\"initial_weights_vgg.h5\")\n",
    "model4.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h4=model4.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model4.save(\"adam_vgg_5.h5\")\n",
    "\n",
    "model5 = model_from_json(loaded_model_json)\n",
    "model5.load_weights(\"initial_weights_vgg.h5\")\n",
    "model5.compile(optimizer=Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h5=model5.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model5.save(\"adam_vgg_6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "\n",
    "model6 = model_from_json(loaded_model_json)\n",
    "model6.load_weights(\"initial_weights_vgg.h5\")\n",
    "model6.compile(optimizer=Adagrad(lr=0.002,  decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h6=model6.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model6.save(\"adag_vgg_7.h5\")\n",
    "\n",
    "model7 = model_from_json(loaded_model_json)\n",
    "model7.load_weights(\"initial_weights_vgg.h5\")\n",
    "model7.compile(optimizer=Adagrad(lr=0.001,  decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h7=model7.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model7.save(\"adag_vgg_8.h5\")\n",
    "\n",
    "model8 = model_from_json(loaded_model_json)\n",
    "model8.load_weights(\"initial_weights_vgg.h5\")\n",
    "model8.compile(optimizer=Adagrad(lr=0.0005,  decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "h8=model8.fit(train_features, train_labels, batch_size=batch_size, epochs=50, validation_data=(valid_features, valid_labels))\n",
    "model8.save(\"adag_vgg_9.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#Available:h3-h8\n",
    "\n",
    "#To show that parameters learnt are quantitatively different : Lead to difference in test values\n",
    "#test1_labels = h.model.predict(test_features)\n",
    "#test2_labels = h1.model.predict(test_features)\n",
    "#test3_labels = h2.model.predict(test_features)\n",
    "\n",
    "#test3_labels = h3.model.predict(test_features)\n",
    "#test4_labels = h4.model.predict(test_features)\n",
    "#test5_labels = h5.model.predict(test_features)\n",
    "\n",
    "#test6_labels = h6.model.predict(test_features)\n",
    "#test7_labels = h7.model.predict(test_features)\n",
    "#test8_labels = h8.model.predict(test_features)\n",
    "\n",
    "test_actual = to_categorical(test_labels)\n",
    "\n",
    "\n",
    "#print np.mean((test1_labels - test_actual)**2)\n",
    "#print np.mean((test2_labels - test_actual)**2)\n",
    "#print np.mean((test3_labels - test_actual)**2)\n",
    "\n",
    "\n",
    "#print np.mean((test3_labels - test_actual)**2)\n",
    "#print np.mean((test4_labels - test_actual)**2)\n",
    "#print np.mean((test5_labels - test_actual)**2)\n",
    "\n",
    "#print np.mean((test6_labels - test_actual)**2)\n",
    "#print np.mean((test7_labels - test_actual)**2)\n",
    "#print np.mean((test8_labels - test_actual)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.model.evaluate(test_features,test_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Training Error\n",
    "#trn_loss_run1 = [h.history['acc'][-1],h3.history['acc'][-1],h6.history['acc'][-1]]\n",
    "#trn_loss_run2 = [h1.history['acc'][-1],h4.history['acc'][-1],h7.history['acc'][-1]]\n",
    "#trn_loss_run3 = [h2.history['acc'][-1],h5.history['acc'][-1],h8.history['acc'][-1]]\n",
    "\n",
    "trn_loss_run1 = [0.9892,0.9741,0.9804]\n",
    "trn_loss_run2 = [0.9876,0.9817,0.9988]\n",
    "trn_loss_run3 = [0.9784,0.9882,0.9941]\n",
    "labels = ['SGD', 'Adam', 'Adagrad']\n",
    "\n",
    "plt.figure(figsize=(5, 3))  \n",
    "plt.scatter(labels,trn_loss_run1)\n",
    "plt.scatter(labels,trn_loss_run2)\n",
    "plt.scatter(labels,trn_loss_run3)\n",
    "plt.title('Training Accuracy')\n",
    "plt.ylabel('trn_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Testing Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_loss_run1 = [model0.evaluate(test_features,test_actual)[1],model3.evaluate(test_features,test_actual)[1],model6.evaluate(test_features,test_actual)[1]]\n",
    "tst_loss_run2 = [model1.evaluate(test_features,test_actual)[1],model4.evaluate(test_features,test_actual)[1], model7.evaluate(test_features,test_actual)[1]]\n",
    "tst_loss_run3 = [model2.evaluate(test_features,test_actual)[1],model5.evaluate(test_features,test_actual)[1],model8.evaluate(test_features,test_actual)[1]]\n",
    "labels = ['SGD', 'Adam', 'Adagrad']\n",
    "\n",
    "plt.figure(figsize=(5, 3))  \n",
    "plt.scatter(labels,tst_loss_run1)\n",
    "plt.scatter(labels,tst_loss_run2)\n",
    "plt.scatter(labels,tst_loss_run3)\n",
    "plt.title('Testing Accuracy for VGG16')\n",
    "plt.ylabel('test_acc')\n",
    "plt.yticks(np.arange(0.915, 0.96, 0.005))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To show that parameters learnt are quantitatively different : Lead to difference in test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model0 = load_model('sgd_vgg_1.h5')\n",
    "model3 = load_model('adam_vgg_4.h5')\n",
    "model6 = load_model('adag_vgg_7.h5')\n",
    "\n",
    "\n",
    "test0_labels = model0.predict(test_features)\n",
    "test3_labels = model3.predict(test_features)\n",
    "test6_labels = model6.predict(test_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('sgd_vgg_2.h5')\n",
    "model4 = load_model('adam_vgg_5.h5')\n",
    "model7 = load_model('adag_vgg_8.h5')\n",
    "\n",
    "\n",
    "\n",
    "test1_labels = model1.predict(test_features)\n",
    "test4_labels = model4.predict(test_features)\n",
    "test7_labels = model7.predict(test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('sgd_vgg_3.h5')\n",
    "model5 = load_model('adam_vgg_6.h5')\n",
    "model8 = load_model('adag_vgg_9.h5')\n",
    "\n",
    "test2_labels = model2.predict(test_features)\n",
    "test5_labels = model5.predict(test_features)\n",
    "test8_labels = model8.predict(test_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model_vgg.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_loss_sgd_adam = [np.linalg.norm((test0_labels - test3_labels)),np.linalg.norm((test0_labels - test6_labels)),np.linalg.norm((test3_labels - test6_labels)) ]\n",
    "dist_loss_sgd_adag = [np.linalg.norm((test1_labels - test4_labels)),np.linalg.norm((test1_labels - test7_labels)),np.linalg.norm((test4_labels - test7_labels)) ]\n",
    "dist_loss_adag_adam =[np.linalg.norm((test2_labels - test5_labels)),np.linalg.norm((test2_labels - test8_labels)),np.linalg.norm((test5_labels - test8_labels)) ]\n",
    "labels = ['SGD-Adam', 'SGD-Adagrad', 'Adam-Adagrad']\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.scatter(labels,dist_loss_sgd_adam)\n",
    "plt.scatter(labels,dist_loss_sgd_adag)\n",
    "plt.scatter(labels,dist_loss_adag_adam)\n",
    "plt.title('Distance between weights for different optimizers')\n",
    "plt.ylabel('distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_loss_run1 = [h.model.evaluate(test_features,test_actual)[1],h3.model.evaluate(test_features,test_actual)[1],h6.model.evaluate(test_features,test_actual)[1]]\n",
    "tst_loss_run2 = [h1.model.evaluate(test_features,test_actual)[1],h4.model.evaluate(test_features,test_actual)[1], h7.model.evaluate(test_features,test_actual)[1]]\n",
    "tst_loss_run3 = [h2.model.evaluate(test_features,test_actual)[1],h5.model.evaluate(test_features,test_actual)[1],h8.model.evaluate(test_features,test_actual)[1]]\n",
    "labels = ['SGD', 'Adam', 'Adagrad']\n",
    "\n",
    "plt.figure(figsize=(5, 3))  \n",
    "plt.scatter(labels,tst_loss_run1)\n",
    "plt.scatter(labels,tst_loss_run2)\n",
    "plt.scatter(labels,tst_loss_run3)\n",
    "plt.title('Testing Accuracy for VGG16')\n",
    "plt.ylabel('test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model_vgg.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To show that parameters learnt are quantitatively different : Lead to difference in test values\n",
    "#SGD-vs-Adam\n",
    "alpha = np.arange(0, 1.01, 0.01)\n",
    "loss1 = np.zeros((len(alpha),))\n",
    "loss2 = np.zeros((len(alpha),))\n",
    "loss3 = np.zeros((len(alpha),))\n",
    "\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model13 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model0.get_weights()]\n",
    "    s = [(1-a) * w for w in model3.get_weights()]\n",
    "    model13.set_weights([x+y for x,y in zip(f,s)])\n",
    "    model13.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss1[i] = model13.evaluate(test_features,test_actual)[0]\n",
    "    i +=1\n",
    "\n",
    "    \n",
    "i=0    \n",
    "for a in alpha:\n",
    "    model13 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model1.get_weights()]\n",
    "    s = [(1-a) * w for w in model4.get_weights()]\n",
    "    model13.set_weights([x+y for x,y in zip(f,s)])\n",
    "    model13.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss2[i] = model13.evaluate(test_features,test_actual)[0]\n",
    "    i +=1\n",
    "\n",
    "i=0    \n",
    "for a in alpha:\n",
    "    model13 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model2.get_weights()]\n",
    "    s = [(1-a) * w for w in model5.get_weights()]\n",
    "    model13.set_weights([x+y for x,y in zip(f,s)])\n",
    "    model13.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss3[i] = model13.evaluate(test_features,test_actual)[0]\n",
    "    i +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(alpha,loss1,'r--',alpha,loss2,'b--',alpha,loss3,'g--')\n",
    "plt.legend(['(p*sgd + (1-p)*adam)'], loc='upper right')  \n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alpha,loss1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam-vs-Adagrad\n",
    "#Adam-vs-Adagrad\n",
    "\n",
    "alpha = np.arange(0, 1.05, 0.05)\n",
    "loss1 = np.zeros((len(alpha),))\n",
    "\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model36 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model3.get_weights()]\n",
    "    s = [(1-a) * w for w in model6.get_weights()]\n",
    "    model36.set_weights([x+y for x,y in zip(f,s)])\n",
    "    #t36_labels = model36.predict(test_features)\n",
    "    model36.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss1[i] = model36.evaluate(test_features,test_actual)[0]\n",
    "    i +=1\n",
    "\n",
    "\n",
    "    \n",
    "loss2 = np.zeros((len(alpha),))\n",
    "loss3 = np.zeros((len(alpha),))\n",
    "\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model36 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model4.get_weights()]\n",
    "    s = [(1-a) * w for w in model7.get_weights()]\n",
    "    model36.set_weights([x+y for x,y in zip(f,s)])\n",
    "    #t36_labels = model36.predict(test_features)\n",
    "    model36.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss2[i] = model36.evaluate(test_features,test_actual)[0]\n",
    "    i +=1\n",
    "    \n",
    "    \n",
    "    \n",
    "i=0\n",
    "for a in alpha:\n",
    "    model36 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model5.get_weights()]\n",
    "    s = [(1-a) * w for w in model8.get_weights()]\n",
    "    model36.set_weights([x+y for x,y in zip(f,s)])\n",
    "    #t36_labels = model36.predict(test_features)\n",
    "    model36.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss3[i] = model36.evaluate(test_features,test_actual)[0]\n",
    "    i +=1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(alpha,loss1,'r--',alpha,loss2,'b--',alpha,loss3,'g--')\n",
    "plt.legend(['(p*adam + (1-p)*adagrad)'], loc='upper right')  \n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD-vs-Adagrad\n",
    "\n",
    "alpha = np.arange(0, 1.01, 0.01)\n",
    "loss1 = np.zeros((len(alpha),))\n",
    "loss2 = np.zeros((len(alpha),))\n",
    "loss3 = np.zeros((len(alpha),))\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model0.get_weights()]\n",
    "    s = [(1-a) * w for w in model6.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    #t16_labels = model16.predict(test_features)\n",
    "    model16.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss1[i] = model16.evaluate(test_features,test_actual)[0]\n",
    "    \n",
    "    i +=1\n",
    "    \n",
    "i=0    \n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model1.get_weights()]\n",
    "    s = [(1-a) * w for w in model7.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    #t16_labels = model16.predict(test_features)\n",
    "    model16.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss2[i] = model16.evaluate(test_features,test_actual)[0]\n",
    "    \n",
    "    i +=1\n",
    "\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model2.get_weights()]\n",
    "    s = [(1-a) * w for w in model8.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    #t16_labels = model16.predict(test_features)\n",
    "    model16.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss3[i] = model16.evaluate(test_features,test_actual)[0]\n",
    "    \n",
    "    i +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(alpha,loss1,'r--',alpha,loss2,'b--',alpha,loss3,'g--')\n",
    "plt.legend(['(p*sgd + (1-p)*adagrad)'], loc='upper right')  \n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model16 = model_from_json(loaded_model_json)\n",
    "f = [0.4 * w for w in model0.get_weights()]\n",
    "s = [(0.6) * w for w in model6.get_weights()]\n",
    "model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "#t16_labels = model16.predict(test_features)\n",
    "model16.compile(optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.0),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "loss_smaller_test = model16.evaluate(test_features,test_actual)[0]\n",
    "loss_smaller_valid = model16.evaluate(valid_features,valid_labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_smaller_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD-vs-Adam\n",
    "\n",
    "alpha = np.arange(0, 1.05, 0.05)\n",
    "loss1 = np.zeros((len(alpha),))\n",
    "loss2 = np.zeros((len(alpha),))\n",
    "loss3 = np.zeros((len(alpha),))\n",
    "loss4 = np.zeros((len(alpha),))\n",
    "loss5 = np.zeros((len(alpha),))\n",
    "loss6 = np.zeros((len(alpha),))\n",
    "\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model0.get_weights()]\n",
    "    s = [(1-a) * w for w in model3.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss1[i] = np.linalg.norm(t16_labels - test0_labels)\n",
    "    loss2[i] = np.linalg.norm(t16_labels - test3_labels)\n",
    "    i +=1\n",
    "\n",
    "    \n",
    "\n",
    "###########\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model1.get_weights()]\n",
    "    s = [(1-a) * w for w in model4.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss3[i] = np.linalg.norm(t16_labels - test1_labels)\n",
    "    loss4[i] = np.linalg.norm(t16_labels - test4_labels)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model2.get_weights()]\n",
    "    s = [(1-a) * w for w in model5.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss5[i] = np.linalg.norm(t16_labels - test2_labels)\n",
    "    loss6[i] = np.linalg.norm(t16_labels - test5_labels)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(alpha,loss1,'r--',alpha,loss2,'r:',alpha,loss3,'b--',alpha,loss4,'b:',alpha,loss5,'g--',alpha,loss6,'g:')\n",
    "plt.legend(['sgd-vs-(p*sgd + (1-p)*adam)', 'adam-vs-(p*sgd + (1-p)*adam)'], loc='upper right') \n",
    "plt.ylabel(\"distance\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam-vs-Adagrad\n",
    "\n",
    "alpha = np.arange(0, 1.05, 0.05)\n",
    "loss1 = np.zeros((len(alpha),))\n",
    "loss2 = np.zeros((len(alpha),))\n",
    "loss3 = np.zeros((len(alpha),))\n",
    "loss4 = np.zeros((len(alpha),))\n",
    "loss5 = np.zeros((len(alpha),))\n",
    "loss6 = np.zeros((len(alpha),))\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model3.get_weights()]\n",
    "    s = [(1-a) * w for w in model6.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss1[i] = np.linalg.norm(t16_labels - test3_labels)\n",
    "    loss2[i] = np.linalg.norm(t16_labels - test6_labels)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "###########\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model4.get_weights()]\n",
    "    s = [(1-a) * w for w in model7.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss3[i] = np.linalg.norm(t16_labels - test4_labels)\n",
    "    loss4[i] = np.linalg.norm(t16_labels - test7_labels)\n",
    "    i +=1\n",
    "\n",
    "################\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model5.get_weights()]\n",
    "    s = [(1-a) * w for w in model8.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss5[i] = np.linalg.norm(t16_labels - test5_labels)\n",
    "    loss6[i] = np.linalg.norm(t16_labels - test8_labels)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(alpha,loss1,'r--',alpha,loss2,'r:',alpha,loss3,'b--',alpha,loss4,'b:',alpha,loss5,'g--',alpha,loss6,'g:')\n",
    "plt.legend(['adam-vs-(p*adam + (1-p)*adagrad)', 'adagrad-vs-(p*adam + (1-p)*adagrad)'], loc='upper right') \n",
    "plt.ylabel(\"distance\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(alpha,loss5,'g--',alpha,loss6,'g:')\n",
    "plt.legend(['adam-vs-(p*adam + (1-p)*adagrad)', 'adagrad-vs-(p*adam + (1-p)*adagrad)'], loc='upper right') \n",
    "plt.ylabel(\"distance\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD-vs-Adagrad\n",
    "\n",
    "alpha = np.arange(0, 1.05, 0.05)\n",
    "loss1 = np.zeros((len(alpha),))\n",
    "loss2 = np.zeros((len(alpha),))\n",
    "loss3 = np.zeros((len(alpha),))\n",
    "loss4 = np.zeros((len(alpha),))\n",
    "loss5 = np.zeros((len(alpha),))\n",
    "loss6 = np.zeros((len(alpha),))\n",
    "\n",
    "\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model0.get_weights()]\n",
    "    s = [(1-a) * w for w in model6.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss1[i] = np.linalg.norm(t16_labels - test0_labels)\n",
    "    loss2[i] = np.linalg.norm(t16_labels - test6_labels)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model1.get_weights()]\n",
    "    s = [(1-a) * w for w in model7.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss3[i] = np.linalg.norm(t16_labels - test1_labels)\n",
    "    loss4[i] = np.linalg.norm(t16_labels - test7_labels)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "i=0\n",
    "for a in alpha:\n",
    "    model16 = model_from_json(loaded_model_json)\n",
    "    f = [a * w for w in model2.get_weights()]\n",
    "    s = [(1-a) * w for w in model8.get_weights()]\n",
    "    model16.set_weights([x+y for x,y in zip(f,s)])\n",
    "    t16_labels = model16.predict(test_features)\n",
    "    loss5[i] = np.linalg.norm(t16_labels - test2_labels)\n",
    "    loss6[i] = np.linalg.norm(t16_labels - test8_labels)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(alpha,loss1,'r--',alpha,loss2,'r:',alpha,loss3,'b--',alpha,loss4,'b:',alpha,loss5,'g--',alpha,loss6,'g:')\n",
    "plt.legend(['sgd-vs-(p*sgd + (1-p)*adagrad)', 'adagrad-vs-(p*sgd + (1-p)*adagrad)'], loc='upper right') \n",
    "plt.ylabel(\"distance\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
